{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8fhYo3UAuMk"
      },
      "outputs": [],
      "source": [
        "# Included Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (RandomForestClassifier,\n",
        "                             GradientBoostingClassifier)\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve,\n",
        "                             confusion_matrix)\n",
        "from scipy import stats\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9i6tQTaxUf"
      },
      "source": [
        "## Data Preprocessing and Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EDB_if9v-w8e",
        "outputId": "6748b429-1caf-464a-ebf7-ea5d77d2d643"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcU4zOg8QK_",
        "outputId": "87993928-07ff-49a8-ace8-ad032b69600b"
      },
      "outputs": [],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4irHGkfaxUg"
      },
      "outputs": [],
      "source": [
        "# Filter for Tech Employees only\n",
        "df = df[df['tech_company'] == \"Yes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gIlUTZ0mUTl",
        "outputId": "38e2329b-7c6c-45e6-c243-991ebda7a8e1"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2GkQOxC3RMW"
      },
      "outputs": [],
      "source": [
        "# Keep only relevant columns\n",
        "\n",
        "df = df[['workplace_resources','mh_employer_discussion','medical_coverage',\n",
        "         'mh_share','gender','age']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4K6Y8hU3ZcN"
      },
      "outputs": [],
      "source": [
        "# Drop missing values\n",
        "\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgiJSwNlp2D_"
      },
      "outputs": [],
      "source": [
        "# Data Preparation\n",
        "\n",
        "df['high_comfort'] = np.where(df['mh_share'] >= 7, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On40szN3p4uP"
      },
      "outputs": [],
      "source": [
        "# Encode predictors\n",
        "\n",
        "df['resources_binary'] = df['workplace_resources'].map({'Yes':1,'No':0})\n",
        "# -1 for \"I don't know\"\n",
        "df['resources_binary'] = df['resources_binary'].fillna(-1)\n",
        "\n",
        "df['employer_binary'] = df['mh_employer_discussion'].map({'Yes':1,'No':0})\n",
        "df['employer_binary'] = df['employer_binary'].fillna(-1)\n",
        "\n",
        "df['coverage_binary']  = df['medical_coverage'].map({'Yes':1,'No':0})\n",
        "df['coverage_binary']  = df['coverage_binary'].fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gJkcD__p7ES",
        "outputId": "b7489f9b-a86a-45ff-b5f8-bdd2fe570da8"
      },
      "outputs": [],
      "source": [
        "## Engineered variable: Combined support\n",
        "\n",
        "df['combined_support'] = ((df['resources_binary'] +\n",
        "                           df['employer_binary'] +\n",
        "                           df['coverage_binary']) >= 2).astype(int)\n",
        "\n",
        "print(\"Dataset shape after cleaning:\", df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6vNezGsMUP9"
      },
      "outputs": [],
      "source": [
        "# Encode gender as binary for modeling (Female=1, Male=0, Others=-1)\n",
        "df['gender_binary'] = df['gender'].map({'Female':1, 'Male':0}).fillna(-1)\n",
        "\n",
        "# Normalize age\n",
        "df['age_scaled'] = (df['age'] - df['age'].mean()) / df['age'].std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnz195NeaxUi"
      },
      "source": [
        "## Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ei5X4a4jaxUi",
        "outputId": "ec99183c-57b8-48f1-b830-124500edf69d"
      },
      "outputs": [],
      "source": [
        "# Display the descriptive statistices of the cleaned data\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvXzftMZqBnS"
      },
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "# Function to add percentages on bar plots\n",
        "def plot_count_with_pct(x, hue, title):\n",
        "    ax = sns.countplot(x=x, hue=hue, data=df, palette=\"Set2\")\n",
        "    total = len(df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.annotate(f'{100*height/total:.1f}%',\n",
        "                    (p.get_x() + p.get_width()/2, height),\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x.replace('_', ' ').replace('mh', 'Mental Health').title())\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend(title=\"High Comfort\", labels=[\"No (0)\", \"Yes (1)\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NLqwnZH_BgyX",
        "outputId": "a47a2539-cc4f-4fa0-e1fb-cb6821b8c12a"
      },
      "outputs": [],
      "source": [
        "plot_count_with_pct(\"workplace_resources\",\"high_comfort\",\"Comfort vs\"\n",
        "                    \"Workplace Resources\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Mw-9wf20BhTa",
        "outputId": "ac860fe3-105a-4277-9d6c-a2ae09c5b82c"
      },
      "outputs": [],
      "source": [
        "plot_count_with_pct(\"mh_employer_discussion\",\"high_comfort\",\"Comfort vs\"\n",
        "                    \"Employer Discussion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9Ajs2wf5BjXO",
        "outputId": "da377f48-e1ab-48f8-b2b5-ca8cd9779d47"
      },
      "outputs": [],
      "source": [
        "plot_count_with_pct(\"medical_coverage\",\"high_comfort\",\"Comfort vs\"\n",
        "                    \"Medical Coverage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "AoVTK8spqJNO",
        "outputId": "6106a950-f17e-4ff9-8ab4-d33d654fe9d8"
      },
      "outputs": [],
      "source": [
        "# Boxplot comfort levels\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"workplace_resources\", y=\"mh_share\", data=df,\n",
        "            hue=\"workplace_resources\", palette=\"pastel\",legend=False)\n",
        "plt.ylabel('Mental Health Share')\n",
        "plt.xlabel(\"workplace_resources\".replace('_', ' ')\n",
        "                                .replace('mh', 'Mental Health').title())\n",
        "plt.title(\"Comfort Scores by Workplace Resources\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "0Ru2UZWCqMzL",
        "outputId": "d5dcc060-abbe-40cd-d877-983c9e174086"
      },
      "outputs": [],
      "source": [
        "# Histogram distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df['mh_share'], bins=10, kde=False, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Comfort Sharing Scores\")\n",
        "plt.xlabel(\"Comfort Sharing Score (mh_share)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "wXPvD7zJqP4P",
        "outputId": "6f94e5e0-776d-4f40-e82c-45dd35c9ce18"
      },
      "outputs": [],
      "source": [
        "# Stratified by gender\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"gender\", y=\"high_comfort\", hue=\"workplace_resources\",\n",
        "            data=df, errorbar=None, palette=\"pastel\", edgecolor=\"black\")\n",
        "plt.title(\"High Comfort by Gender & Resources\")\n",
        "plt.ylabel(\"% High Comfort\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "XM61ozmJPsK8",
        "outputId": "96e15862-5bb7-4a16-9ef2-7c97f26a9026"
      },
      "outputs": [],
      "source": [
        "# Histogram of Age\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(df['age'], bins=20, color=\"skyblue\", edgecolor='black')\n",
        "plt.title(\"Distribution of Age\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "df['age_group'] = pd.cut(df['age'], bins=[18,30,40,50,60,80],\n",
        "                         labels=[\"18-29\",\"30-39\",\"40-49\",\"50-59\",\"60+\"])\n",
        "sns.barplot(x=\"age_group\", y=\"mh_share\", data=df,\n",
        "            color=\"skyblue\", errorbar=None)\n",
        "plt.title(\"Average Comfort Score by Age Group\")\n",
        "plt.ylabel(\"Mean Comfort Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MILxR5SFrl9D"
      },
      "outputs": [],
      "source": [
        "# Average Comfort Score by Age Group\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjZ1mCrpaxUj"
      },
      "source": [
        "## Independent T-Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo_Gedv5qXV4"
      },
      "outputs": [],
      "source": [
        "# Calculate Confidence interval for the Mean Difference\n",
        "def get_confidence_interval(yes, no) -> tuple[float, float, float]:\n",
        "    # Retrieve the mean of the two groups\n",
        "    mean_diff = yes.mean() - no.mean()\n",
        "    # Compute standard error\n",
        "    se = np.sqrt(yes.var(ddof=1)/len(yes) + no.var(ddof=1)/len(no))\n",
        "    # Calculate the 95% confidence interval\n",
        "    ci_low, ci_high = mean_diff - 1.96*se, mean_diff + 1.96*se\n",
        "    return mean_diff, ci_low, ci_high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4hdVgpbaxUk"
      },
      "outputs": [],
      "source": [
        "# Statistical Testing : Perform Welch's Independent T-Test\n",
        "def welch_test(df, col1, col2, label):\n",
        "    # Extract the data for the two comparing columns\n",
        "    yes = df.loc[df[col1] == 1, col2].values\n",
        "    no = df.loc[df[col1] == 0, col2].values\n",
        "    # Perform Welch's T-Test\n",
        "    t_stat, p_val = stats.ttest_ind(yes, no, equal_var=False)\n",
        "    # Retrieve the significance test stats\n",
        "    mean_diff, ci_low, ci_high = get_confidence_interval(yes, no)\n",
        "    return {\n",
        "            \"Variable\": label,\n",
        "            \"t(df)\": f\"{t_stat:.3f}\",\n",
        "            \"p\": p_val,\n",
        "            \"Mean Diff\": f\"{mean_diff:.2f}\",\n",
        "            \"95% CI\": f\"[{ci_low:.2f}, {ci_high:.2f}]\",\n",
        "            \"Significance\": \"Sig.\" if p_val < 0.05 else \"Not Sig.\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "yck_Da4CaxUk",
        "outputId": "4e97d932-b0b5-4c0f-b05d-6c7126f71d8f"
      },
      "outputs": [],
      "source": [
        "# Perfrom T-Test on all six predictors\n",
        "rows = []\n",
        "rows.append(welch_test(df, \"resources_binary\", \"mh_share\",\n",
        "                      \"Resources Binary\"))\n",
        "rows.append(welch_test(df, \"employer_binary\", \"mh_share\",\n",
        "                       \"Employer Discussion\"))\n",
        "rows.append(welch_test(df, \"coverage_binary\", \"mh_share\",\n",
        "                       \"Coverage Binary\"))\n",
        "table = pd.DataFrame(rows, columns=[\"Variable\", \"t(df)\", \"p\", \"Mean Diff\",\n",
        "                                    \"95% CI\", \"Significance\"])\n",
        "\n",
        "# Report Test Results for all six predictors\n",
        "display(table.style.set_caption(\"Table 3. Independent Samples t-Tests for\"\n",
        "                                \"Mental Health Sharing (mh_share)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjwj_LXaxUk"
      },
      "source": [
        "## Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Olq4DYaxUk"
      },
      "outputs": [],
      "source": [
        "# Structures to store Test and Train Set Results\n",
        "probs_test, preds_test = {}, {}\n",
        "probs_train, preds_train = {}, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0LkSPS7axUl"
      },
      "outputs": [],
      "source": [
        "# Print Model Information\n",
        "def evaluate_model(y_test, y_pred, y_prob, model_name, total = 0):\n",
        "    if total == 0:\n",
        "        print(f\"\\n{model_name} Regression Performance:\")\n",
        "        print(\"Accuracy:\", round(accuracy_score(y_test,y_pred),4))\n",
        "        print(\"Precision:\", round(precision_score(y_test,y_pred),4))\n",
        "        print(\"Recall:\", round(recall_score(y_test,y_pred),4))\n",
        "        print(\"F1 Score:\", round(f1_score(y_test,y_pred),4))\n",
        "        print(\"AUC:\", round(roc_auc_score(y_test,y_prob),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8wVp-ePaxUl"
      },
      "source": [
        "### Baseline Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRywFjK6axUl",
        "outputId": "404e2070-6aee-45fa-86be-209b4b39a6f0"
      },
      "outputs": [],
      "source": [
        "# Baseline Logistic Regression\n",
        "model_name = \"Baseline Logistic\"\n",
        "\n",
        "# Test-Train Split\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X1 = df[['resources_binary']].dropna()\n",
        "y = df[\"high_comfort\"].astype(int).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X1, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Fit Logistic Regression Model on the training data\n",
        "logit1 = LogisticRegression(max_iter=1000)\n",
        "logit1.fit(X_train, y_train)\n",
        "\n",
        "# Predict Train and Test Sets\n",
        "# Predict binary class and probabilities labels for test set\n",
        "pred_test1 = logit1.predict(X_test)\n",
        "prob_test1 = logit1.predict_proba(X_test)[:,1]\n",
        "pred_train1 = logit1.predict(X_train)\n",
        "prob_train1 = logit1.predict_proba(X_train)[:,1]\n",
        "\n",
        "# Store Train and Test Set Results\n",
        "probs_train[model_name] = prob_train1\n",
        "preds_train[model_name] = pred_train1\n",
        "probs_test[model_name] = prob_test1\n",
        "preds_test[model_name] = pred_test1\n",
        "\n",
        "# Display Test Set Result\n",
        "evaluate_model(y_test, pred_test1, prob_test1, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kASuRPxJaxUm"
      },
      "source": [
        "### Improved Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY4KnAP7q9cq",
        "outputId": "4df00140-f253-4fa7-fc63-77233ab7e866"
      },
      "outputs": [],
      "source": [
        "# Improved Logistic Regression (with demographics)\n",
        "model_name = \"Improved Logistic\"\n",
        "\n",
        "# Test-Train Split for all 6 predictors\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X2 = df[['resources_binary','employer_binary','coverage_binary',\n",
        "         'combined_support','gender_binary','age_scaled']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X2,y,test_size=0.2,stratify=y,random_state=42\n",
        ")\n",
        "\n",
        "# Fit the Improved Logistic Regression Model on the training data\n",
        "logit2 = LogisticRegression(max_iter=1000)\n",
        "logit2.fit(X_train,y_train)\n",
        "\n",
        "# Predict Train and Test Sets\n",
        "# Predict binary class and probabilities labels for test set\n",
        "pred_test2 = logit2.predict(X_test)\n",
        "prob_test2 = logit2.predict_proba(X_test)[:,1]\n",
        "pred_train2 = logit2.predict(X_train)\n",
        "prob_train2 = logit2.predict_proba(X_train)[:,1]\n",
        "\n",
        "# Store Train and Test Set Results\n",
        "probs_train[model_name] = prob_train2\n",
        "preds_train[model_name] = pred_train2\n",
        "probs_test[model_name] = prob_test2\n",
        "preds_test[model_name] = pred_test2\n",
        "\n",
        "# Display Test Set Result\n",
        "evaluate_model(y_test, pred_test2, prob_test2, model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDob9dyaaxUm"
      },
      "outputs": [],
      "source": [
        "def pred_summary(X: pd.DataFrame, y: pd.Series):\n",
        "        Xc = sm.add_constant(X)\n",
        "        logit = sm.Logit(y, Xc)\n",
        "        result = logit.fit(disp=False)\n",
        "        summ = result.summary2().tables[1].copy()\n",
        "        summ = summ.rename(columns={\"Coef.\": \"β\", \"Std.Err.\": \"SE\", \"P>|z|\"\n",
        "                                    : \"p\", \"[0.025\": \"CI Low\", \"0.975]\"\n",
        "                                    :\"CI High\"})\n",
        "        summ[\"OR\"] = np.exp(summ[\"β\"])\n",
        "        # Order columns\n",
        "        summ = summ[[\"β\", \"SE\", \"OR\", \"CI Low\", \"CI High\", \"p\"]]\n",
        "        return summ.round(3), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "mvUccV46axUm",
        "outputId": "f7cc623b-1a40-4c80-dea6-102fa0318be5"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Coefficients\n",
        "predictors = list(X_train.columns)\n",
        "coefficients = pd.DataFrame({\n",
        "    'Predictor': predictors,\n",
        "    'Coefficient': logit2.coef_[0]\n",
        "})\n",
        "coefficients['Odds_Ratio'] = np.exp(coefficients['Coefficient'])\n",
        "coefficients.sort_values(by='Odds_Ratio', ascending=False, inplace=True)\n",
        "display(coefficients.style.set_caption(\"Table 4. Logistic Regression\"\n",
        "                                       \"Coefficients and Odds Ratios \"))\n",
        "\n",
        "sm_table, sm_result = pred_summary(X_train, y_train)\n",
        "display(sm_table.style.set_caption(\"Table 5. Predictor Logit\"\n",
        "                                   \"Estimates (Train Set)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3CB6mrSaxUm"
      },
      "source": [
        "### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nanz9BOrFmS",
        "outputId": "2a3313f8-5ea1-4c79-efd9-55a1696e7e43"
      },
      "outputs": [],
      "source": [
        "# Supporting Model: Random Forest\n",
        "model_name = \"Random Forest\"\n",
        "\n",
        "# Fit Random Forest Model\n",
        "rf = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\",\n",
        "                            random_state=42)\n",
        "rf.fit(X_train,y_train)\n",
        "\n",
        "# Predict Train and Test Sets\n",
        "pred_test3 = rf.predict(X_test)\n",
        "prob_test3 = rf.predict_proba(X_test)[:,1]\n",
        "pred_train3 = rf.predict(X_train)\n",
        "prob_train3 = rf.predict_proba(X_train)[:,1]\n",
        "\n",
        "# Store Train and Test Set Results\n",
        "probs_train[model_name] = prob_train3\n",
        "preds_train[model_name] = pred_train3\n",
        "probs_test[model_name] = prob_test3\n",
        "preds_test[model_name] = pred_test3\n",
        "\n",
        "# Display Test Set Result\n",
        "evaluate_model(y_test, pred_test3, prob_test3, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP749YqNaxUn"
      },
      "source": [
        "### Gradient Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux6q7JP2N0pN",
        "outputId": "7ba9179d-4cb5-433a-b70f-61671d4d071a"
      },
      "outputs": [],
      "source": [
        "# Supporting Model: Gradient Boosting\n",
        "model_name = \"Gradient Boosting\"\n",
        "\n",
        "# Fit Gradient Boosting Model on the training data\n",
        "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05,\n",
        "                                random_state=42)\n",
        "gb.fit(X_train,y_train)\n",
        "\n",
        "# Predict Train and Test Sets\n",
        "# Predict binary class and probabilities labels for test set\n",
        "pred_test4 = gb.predict(X_test)\n",
        "prob_test4 = gb.predict_proba(X_test)[:,1]\n",
        "pred_train4 = gb.predict(X_train)\n",
        "prob_train4 = gb.predict_proba(X_train)[:,1]\n",
        "\n",
        "# Store Train and Test Set Results\n",
        "probs_train[model_name] = prob_train4\n",
        "preds_train[model_name] = pred_train4\n",
        "probs_test[model_name] = prob_test4\n",
        "preds_test[model_name] = pred_test4\n",
        "\n",
        "# Display Test Set Result\n",
        "evaluate_model(y_test, pred_test4, prob_test4, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usLl2RNCaxUn"
      },
      "source": [
        "### Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "uw8VHEZwrJ4x",
        "outputId": "b6f6a2bd-72a4-4ee2-8abf-54566a283c2c"
      },
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "# Create a pandas series for the feature importances\n",
        "# This gives each feature's contribution to the model\n",
        "rf_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
        "gb_importances = pd.Series(gb.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# Set up figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "# Random Forest plot\n",
        "rf_importances.sort_values().plot(\n",
        "    kind=\"barh\", color=\"teal\", ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Feature Importances – Random Forest\", fontsize=12)\n",
        "axes[0].set_xlabel(\"Importance\")\n",
        "axes[0].set_ylabel(\"Features\")\n",
        "\n",
        "# Gradient Boosting plot\n",
        "gb_importances.sort_values().plot(\n",
        "    kind=\"barh\", color=\"orange\", ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Feature Importances – Gradient Boosting\", fontsize=12)\n",
        "axes[1].set_xlabel(\"Importance\")\n",
        "axes[1].set_ylabel(\"\")  # Hide duplicate y-labels\n",
        "\n",
        "# Adjust layout and display importance plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Bep0SK9trMUQ",
        "outputId": "0b3fcbbc-2519-4292-eac8-fb35bdf7151b"
      },
      "outputs": [],
      "source": [
        "# ROC Comparison\n",
        "# Calculate the false positive rates, true postitive rate and the thresholds for each model\n",
        "fpr1,tpr1,_ = roc_curve(y_test,prob_test1)\n",
        "fpr2,tpr2,_ = roc_curve(y_test,prob_test2)\n",
        "fpr_rf,tpr_rf,_ = roc_curve(y_test,prob_test3)\n",
        "fpr_gb,tpr_gb,_ = roc_curve(y_test,prob_test4)\n",
        "\n",
        "# Plot ROC Curves for each of the models\n",
        "plt.plot(fpr1,tpr1,label=f\"Baseline Logistic (AUC=\"\n",
        "                      f\"{roc_auc_score(y_test,prob_test1):.2f})\",\n",
        "                         color=\"blue\")\n",
        "plt.plot(fpr2,tpr2,label=f\"Improved Logistic (AUC=\"\n",
        "                      f\"{roc_auc_score(y_test,prob_test2):.2f})\",\n",
        "                         color=\"green\")\n",
        "plt.plot(fpr_rf,tpr_rf,label=f\"Random Forest (AUC=\"\n",
        "                      f\"{roc_auc_score(y_test,prob_test3):.2f})\",\n",
        "                         color=\"red\")\n",
        "plt.plot(fpr_gb,tpr_gb,label=f\"Gradient Boosting (AUC=\"\n",
        "                      f\"{roc_auc_score(y_test,prob_test4):.2f})\",\n",
        "                         color=\"orange\")\n",
        "\n",
        "# Plot the lines for random guess and reference\n",
        "plt.plot([0,1],[0,1],\"k--\")\n",
        "\n",
        "# Set the axes lables and titles\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "\n",
        "# Assign legend and display the plots\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WQbMHX3nzg0"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(\n",
        "        y_true: pd.Series, preds: Dict[str, np.ndarray]\n",
        "    ):\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "        axes = axes.flatten()  # Flatten for easier iteration\n",
        "\n",
        "        for ax, (model_name, y_pred) in zip(axes, preds.items()):\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "            # Heatmap with gridlines between cells\n",
        "            sns.heatmap(\n",
        "                cm,\n",
        "                annot=True,\n",
        "                fmt=\"d\",\n",
        "                cmap=\"Blues\",\n",
        "                cbar=True,\n",
        "                ax=ax,\n",
        "                linewidths=1,\n",
        "                linecolor=\"black\",\n",
        "                square=True,\n",
        "                xticklabels=[\"Pred: Low\",\"Pred: High\"],\n",
        "                yticklabels=[\"True: Low\",\"True: High\"]\n",
        "            )\n",
        "\n",
        "            ax.set_title(f\"{model_name}\\nAccuracy = {acc:.2f}\",\n",
        "                         fontsize=11, pad=8)\n",
        "            ax.set_xlabel(\"Predicted Label\")\n",
        "            ax.set_ylabel(\"True Label\")\n",
        "            ax.tick_params(axis='x', labelrotation=45)\n",
        "            ax.tick_params(axis='y', labelrotation=0)\n",
        "\n",
        "        fig.add_artist(plt.Line2D((0.5, 0.5), (0, 1), color='black',\n",
        "                                  linewidth=2,\n",
        "                                  transform=fig.transFigure, zorder=10))\n",
        "        fig.add_artist(plt.Line2D((0, 1), (0.5, 0.5), color='black',\n",
        "                                  linewidth=2,\n",
        "                                  transform=fig.transFigure, zorder=10))\n",
        "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "H5vbE5Esn3EJ",
        "outputId": "cb948739-2290-444e-be99-bad6e856315f"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrices(y_test, preds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPrONHTaxUn"
      },
      "outputs": [],
      "source": [
        "# Train vs Test summaries\n",
        "def performance_table(\n",
        "         y_true: pd.Series, probs: Dict[str, np.ndarray],\n",
        "         preds: Dict[str, np.ndarray]\n",
        "    ) -> pd.DataFrame:\n",
        "        rows = []\n",
        "        # Loop through each model name and retrieve classification metrics\n",
        "        for name in probs.keys():\n",
        "            row = {\n",
        "                \"Model\": name,\n",
        "                \"Accuracy\": accuracy_score(y_true, preds[name]),\n",
        "                \"Precision\": precision_score(y_true, preds[name],\n",
        "                                             zero_division=0),\n",
        "                \"Recall\": recall_score(y_true, preds[name], zero_division=0),\n",
        "                \"F1\": f1_score(y_true, preds[name], zero_division=0),\n",
        "                \"AUC\": roc_auc_score(y_true, probs[name]),\n",
        "            }\n",
        "            rows.append(row)\n",
        "        df = pd.DataFrame(rows).sort_values(\"AUC\", ascending=False)\n",
        "        return (df[[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]]\n",
        "               .round(3).reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "0XAQqlvRaxUo",
        "outputId": "e915bc6e-7ef8-47b8-fe3d-75e78e1478b5"
      },
      "outputs": [],
      "source": [
        "# Model Performance Summary\n",
        "# Train Set\n",
        "train_table = performance_table(y_train, probs_train, preds_train)\n",
        "# Test Set\n",
        "test_table = performance_table(y_test, probs_test, preds_test)\n",
        "\n",
        "# Display both train and test set tables\n",
        "display(train_table.style.set_caption(\"Model Comparison Summary \"\n",
        "                                      \"— Train Set\").format(precision=3))\n",
        "display(test_table.style.set_caption(\"Model Comparison Summary \"\n",
        "                                     \"— Test Set\").format(precision=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "KdXZODhFaxUo",
        "outputId": "433666ef-5578-4a05-c44f-b5d5cb1bfbeb"
      },
      "outputs": [],
      "source": [
        "# Display the heatmap for the train set\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.heatmap(train_table.set_index(\"Model\"), annot=True, fmt=\".2f\",\n",
        "            cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Model Performance Heatmap for Train Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aMlo54mWREtA",
        "outputId": "d339e676-1d52-4b19-b135-e5493c024083"
      },
      "outputs": [],
      "source": [
        "# Display the heatmap for the test set\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.heatmap(test_table.set_index(\"Model\"), annot=True, fmt=\".2f\",\n",
        "            cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Model Performance Heatmap for Test Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eivvvW9I5KAC",
        "outputId": "75dbed89-1996-41b2-d888-4aaa73332653"
      },
      "outputs": [],
      "source": [
        "# Cross-Validation\n",
        "\n",
        "cv_scores = cross_val_score(LogisticRegression(max_iter=1000), X2, y, cv=5,\n",
        "                            scoring=\"roc_auc\")\n",
        "print(f\"\\nCross-validated AUC for Improved Logistic Regression: \"\n",
        "      f\"{cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
        "\n",
        "cv_scores_gb = cross_val_score(GradientBoostingClassifier(n_estimators=200,\n",
        "                                                         learning_rate=0.05,\n",
        "                                                         random_state=42),\n",
        "                               X2, y, cv=5, scoring=\"roc_auc\")\n",
        "print(f\"Cross-validated AUC for Gradient Boosting: {cv_scores_gb.mean():.3f} \"\n",
        "      f\"± {cv_scores_gb.std():.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (my_project_env)",
      "language": "python",
      "name": "my_project_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
