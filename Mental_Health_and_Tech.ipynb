{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYIoqycNAl6m",
        "outputId": "409f3764-438b-4db7-d2e6-db9babbeb5d8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qtuOlMdUM7k"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Mental health in the workplace is an increasingly important issue, especially in the technology industry where job demands are high and stigma often discourages employees from speaking openly. Many companies provide supports such as mental health resources, employer-led discussions, and medical coverage, but the effectiveness of these supports in improving employees’ comfort in discussing mental health is not always clear.\n",
        "\n",
        "In this project, we analyze survey data from technology professionals to examine whether workplace supports influence employees’ comfort in discussing mental health. We focus on three main supports — workplace resources, employer discussion, and medical coverage — along with an engineered feature, combined support, which captures whether an employee has at least two supports available. We also consider demographic effects of gender and age.  \n",
        "\n",
        "Our approach combines exploratory data analysis, hypothesis testing, and predictive modeling to provide both statistical evidence and machine learning insights into the role of workplace supports.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHB7XSoYUUoB"
      },
      "source": [
        "# Hypotheses\n",
        "\n",
        "**Null Hypothesis (H₀):** Workplace supports (resources, employer discussion, medical coverage, and combined support) have no effect on employees’ comfort in discussing mental health.  \n",
        "\n",
        "**Alternative Hypothesis (H₁):** Employees with workplace supports (resources, employer discussion, medical coverage, or multiple combined supports) report significantly higher comfort in discussing mental health compared to those without supports.  \n",
        "\n",
        "We will test this using both traditional statistical methods (t-tests, confidence intervals) and predictive modeling (logistic regression, random forest, gradient boosting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8fhYo3UAuMk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve, confusion_matrix)\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "EDB_if9v-w8e",
        "outputId": "5f7ac2ee-dfcc-439f-a0e4-a8c6bf2a4e84"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/dataset/data.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcU4zOg8QK_",
        "outputId": "e9c166a8-0198-4d03-da82-75a96eb9bc8f"
      },
      "outputs": [],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gIlUTZ0mUTl",
        "outputId": "ee392f7f-a59e-44aa-c372-4d2de4ee7244"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87JVgRfuHK0v"
      },
      "source": [
        "The dataset contains 1,242 observations and 11 variables. Most features are categorical, while `mh_share` provides a numeric comfort score (1–10 scale). This gives us a good balance of workplace support indicators and demographic variables for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2GkQOxC3RMW"
      },
      "outputs": [],
      "source": [
        "# 2. Keep only relevant columns\n",
        "\n",
        "df = df[['workplace_resources','mh_employer_discussion','medical_coverage','mh_share','gender','age']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4K6Y8hU3ZcN"
      },
      "outputs": [],
      "source": [
        "# Drop missing values\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgiJSwNlp2D_"
      },
      "outputs": [],
      "source": [
        "# 3. Data Preparation\n",
        "# Outcome: High comfort (>=7 on a 1-10 scale)\n",
        "df['high_comfort'] = np.where(df['mh_share'] >= 7, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On40szN3p4uP"
      },
      "outputs": [],
      "source": [
        "# Encode predictors\n",
        "# Treat \"I don't know\" as a separate category instead of dropping\n",
        "df['resources_binary'] = df['workplace_resources'].map({'Yes':1,'No':0})\n",
        "df['resources_binary'] = df['resources_binary'].fillna(-1)  # -1 for \"I don't know\"\n",
        "\n",
        "df['employer_binary'] = df['mh_employer_discussion'].map({'Yes':1,'No':0})\n",
        "df['employer_binary'] = df['employer_binary'].fillna(-1)\n",
        "\n",
        "df['coverage_binary']  = df['medical_coverage'].map({'Yes':1,'No':0})\n",
        "df['coverage_binary']  = df['coverage_binary'].fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gJkcD__p7ES",
        "outputId": "42ecf4ec-8790-4271-c12b-a4d208d5fc29"
      },
      "outputs": [],
      "source": [
        "## Engineered variable: Combined support (at least 2 supports)\n",
        "df['combined_support'] = ((df['resources_binary'] +\n",
        "                           df['employer_binary'] +\n",
        "                           df['coverage_binary']) >= 2).astype(int)\n",
        "\n",
        "print(\"Dataset shape after cleaning:\", df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6vNezGsMUP9"
      },
      "outputs": [],
      "source": [
        "# Encode gender as binary for modeling (Female=1, Male=0, Others=-1)\n",
        "df['gender_binary'] = df['gender'].map({'Female':1, 'Male':0}).fillna(-1)\n",
        "\n",
        "# Optional: Normalize age (helps logistic regression stability)\n",
        "df['age_scaled'] = (df['age'] - df['age'].mean()) / df['age'].std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ_Hl2hPUsLB"
      },
      "source": [
        "We reduced the dataset to relevant supports (resources, employer discussion, medical coverage), demographics (gender, age), and the outcome (`mh_share`). Binary encodings were created for each support, and a combined support indicator was added. This ensures predictors are machine-learning ready while retaining interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OvXzftMZqBnS",
        "outputId": "a066d87f-fc84-465a-f5e0-2e58641cc7f3"
      },
      "outputs": [],
      "source": [
        "# 4. Exploratory Data Analysis (EDA)\n",
        "# ================================\n",
        "\n",
        "# Function to add percentages on bar plots\n",
        "def plot_count_with_pct(x, hue, title):\n",
        "    ax = sns.countplot(x=x, hue=hue, data=df, palette=\"Set2\")\n",
        "    total = len(df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.annotate(f'{100*height/total:.1f}%',\n",
        "                    (p.get_x() + p.get_width()/2, height),\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_count_with_pct(\"workplace_resources\",\"high_comfort\",\"Comfort vs Workplace Resources\")\n",
        "plot_count_with_pct(\"mh_employer_discussion\",\"high_comfort\",\"Comfort vs Employer Discussion\")\n",
        "plot_count_with_pct(\"medical_coverage\",\"high_comfort\",\"Comfort vs Medical Coverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "AoVTK8spqJNO",
        "outputId": "8e02b614-38a4-431d-f90d-41746aa7d4e6"
      },
      "outputs": [],
      "source": [
        "# Boxplot comfort levels\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"workplace_resources\", y=\"mh_share\", data=df, hue=\"workplace_resources\", palette=\"coolwarm\", legend=False)\n",
        "plt.title(\"Comfort Scores by Workplace Resources\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "0Ru2UZWCqMzL",
        "outputId": "ce298489-f731-4675-f024-691c8505ac9f"
      },
      "outputs": [],
      "source": [
        "# Histogram distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df['mh_share'], bins=10, kde=False, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Comfort Sharing Scores\")\n",
        "plt.xlabel(\"Comfort Score (mh_share)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "wXPvD7zJqP4P",
        "outputId": "8c0165b6-6425-4be0-c0f0-c5b1861bfa79"
      },
      "outputs": [],
      "source": [
        "# Stratified by gender (extra insight)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"gender\", y=\"high_comfort\", hue=\"workplace_resources\", data=df, errorbar=None, palette=\"muted\")\n",
        "plt.title(\"High Comfort by Gender & Resources\")\n",
        "plt.ylabel(\"% High Comfort\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "XM61ozmJPsK8",
        "outputId": "a564ad5f-efde-45ae-9b50-a6c2a8199f66"
      },
      "outputs": [],
      "source": [
        "# Distribution of Age\n",
        "# Histogram of Age\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df['age'], bins=20, color=\"purple\")\n",
        "plt.title(\"Distribution of Age\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Average Comfort Score by Age Group\n",
        "df['age_group'] = pd.cut(df['age'], bins=[18,30,40,50,60,80], labels=[\"18-29\",\"30-39\",\"40-49\",\"50-59\",\"60+\"])\n",
        "sns.barplot(x=\"age_group\", y=\"mh_share\", data=df, color=\"skyblue\", errorbar=None)\n",
        "plt.title(\"Average Comfort Score by Age Group\")\n",
        "plt.ylabel(\"Mean Comfort Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uoF_tEEVOIg"
      },
      "source": [
        "The exploratory analysis shows that employees with workplace supports consistently report higher comfort in discussing mental health. Median comfort scores are higher among those with resources, and distributions suggest that both employer discussion and medical coverage play roles as well. Gender differences appear small, while age shows slight variation across groups but no extreme differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2XWv3brqT3h",
        "outputId": "9d2222df-6cd5-4635-dc87-afbf515134e7"
      },
      "outputs": [],
      "source": [
        "# 5. Statistical Testing\n",
        "# ================================\n",
        "\n",
        "comfort_yes = df.loc[df['resources_binary']==1,'mh_share']\n",
        "comfort_no  = df.loc[df['resources_binary']==0,'mh_share']\n",
        "\n",
        "t_stat, p_val = stats.ttest_ind(comfort_yes, comfort_no, equal_var=False)\n",
        "print(f\"\\nT-test (resources vs no resources): t={t_stat:.3f}, p={p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo_Gedv5qXV4",
        "outputId": "fab70ded-8072-4bc1-e453-9857d69655a9"
      },
      "outputs": [],
      "source": [
        "# Confidence Interval for mean difference\n",
        "mean_diff = comfort_yes.mean() - comfort_no.mean()\n",
        "se = np.sqrt(comfort_yes.var(ddof=1)/len(comfort_yes) + comfort_no.var(ddof=1)/len(comfort_no))\n",
        "ci_low, ci_high = mean_diff - 1.96*se, mean_diff + 1.96*se\n",
        "print(f\"Mean Difference = {mean_diff:.2f}\")\n",
        "print(f\"95% CI = [{ci_low:.2f}, {ci_high:.2f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaZMQD1CVT1z"
      },
      "source": [
        "A Welch’s t-test found a statistically significant difference between employees with resources vs. without (t = 2.503, p = 0.0125). The estimated mean difference was 0.45 comfort points (95% CI [0.10, 0.80]). Since the confidence interval does not cross zero, we reject the null hypothesis and conclude that workplace resources increase comfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UlzvlbGqZuN"
      },
      "outputs": [],
      "source": [
        "# 6. Modeling\n",
        "# ================================\n",
        "\n",
        "def evaluate_model(y_test, y_pred, y_prob, model_name):\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test,y_pred),4))\n",
        "    print(\"Precision:\", round(precision_score(y_test,y_pred),4))\n",
        "    print(\"Recall:\", round(recall_score(y_test,y_pred),4))\n",
        "    print(\"F1 Score:\", round(f1_score(y_test,y_pred),4))\n",
        "    print(\"AUROC:\", round(roc_auc_score(y_test,y_prob),4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge-p0K7Oqc2v",
        "outputId": "b1e8d857-a8de-43b1-e4ab-f169bedd84cf"
      },
      "outputs": [],
      "source": [
        "# Baseline Logistic Regression\n",
        "# ================================\n",
        "X1 = df[['resources_binary']].dropna()\n",
        "y  = df.loc[X1.index, 'high_comfort']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X1, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "logit1 = LogisticRegression(max_iter=1000)\n",
        "logit1.fit(X_train, y_train)\n",
        "y_pred1 = logit1.predict(X_test)\n",
        "y_prob1 = logit1.predict_proba(X_test)[:,1]\n",
        "\n",
        "evaluate_model(y_test, y_pred1, y_prob1, \"Baseline Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGTxz8MyVYw5"
      },
      "source": [
        "Using only workplace resources as a predictor, logistic regression achieved high recall (1.0) but poor AUROC (0.54). This means the model classifies nearly all employees as \"high comfort,\" capturing positives but also many false alarms. Supports beyond resources are needed for better discrimination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY4KnAP7q9cq",
        "outputId": "12c04d87-66a6-4eb3-b858-155522aad77b"
      },
      "outputs": [],
      "source": [
        "# Improved Logistic Regression (with demographics)\n",
        "# ================================\n",
        "X2 = df[['resources_binary','employer_binary','coverage_binary',\n",
        "         'combined_support','gender_binary','age_scaled']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X2,y,test_size=0.2,stratify=y,random_state=42\n",
        ")\n",
        "\n",
        "logit2 = LogisticRegression(max_iter=1000)\n",
        "logit2.fit(X_train,y_train)\n",
        "y_pred2 = logit2.predict(X_test)\n",
        "y_prob2 = logit2.predict_proba(X_test)[:,1]\n",
        "\n",
        "evaluate_model(y_test,y_pred2,y_prob2,\"Improved Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_CxlMSVafn"
      },
      "source": [
        "Adding employer discussion, coverage, combined support, gender, and age improved performance. AUROC rose to ~0.63, recall remained high (0.77), and precision improved. This shows that multiple supports together better explain comfort levels, though the predictive power is still modest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nanz9BOrFmS",
        "outputId": "aa3a349e-92db-45b3-f2c3-218c5a090216"
      },
      "outputs": [],
      "source": [
        "# Supporting Model: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42)\n",
        "rf.fit(X_train,y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
        "\n",
        "evaluate_model(y_test,y_pred_rf,y_prob_rf,\"Random Forest\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWSNVe0HVgl9"
      },
      "source": [
        "The Random Forest model emphasized precision (0.62) but had lower recall (0.55). This means it is more conservative, predicting high comfort less often but with greater accuracy when it does. Feature importance suggests employer discussion and medical coverage are the strongest predictors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux6q7JP2N0pN",
        "outputId": "675061df-48cc-4880-a121-72e78fc33499"
      },
      "outputs": [],
      "source": [
        "#Gradient Boosting (often stronger than RF/Logit)\n",
        "# ================================\n",
        "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "gb.fit(X_train,y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "y_prob_gb = gb.predict_proba(X_test)[:,1]\n",
        "\n",
        "evaluate_model(y_test,y_pred_gb,y_prob_gb,\"Gradient Boosting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOzQZt6dVhnD"
      },
      "source": [
        "Gradient Boosting achieved the best balance (Accuracy 0.63, Recall 0.81, AUROC ~0.65). It outperformed logistic regression and random forest by capturing non-linear relationships while maintaining stability. This suggests workplace supports interact in ways that simple models partially miss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "uw8VHEZwrJ4x",
        "outputId": "f03b180c-6102-4bd8-f4df-8ac346645c44"
      },
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "# ================================\n",
        "\n",
        "# Random Forest\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
        "feat_importances.plot(kind=\"barh\", color=\"teal\")\n",
        "plt.title(\"Feature Importances – Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_importances = pd.Series(gb.feature_importances_, index=X_train.columns)\n",
        "gb_importances.plot(kind=\"barh\", color=\"orange\")\n",
        "plt.title(\"Feature Importances – Gradient Boosting\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Bep0SK9trMUQ",
        "outputId": "b24025c9-58c4-4be9-9c07-cf1e07d0ecc0"
      },
      "outputs": [],
      "source": [
        "# 7. ROC Comparison\n",
        "# ================================\n",
        "fpr1,tpr1,_ = roc_curve(y_test,y_prob1)   # FIX: remove extra f\n",
        "fpr2,tpr2,_ = roc_curve(y_test,y_prob2)\n",
        "fpr_rf,tpr_rf,_ = roc_curve(y_test,y_prob_rf)\n",
        "fpr_gb,tpr_gb,_ = roc_curve(y_test,y_prob_gb)\n",
        "\n",
        "plt.plot(fpr1,tpr1,label=f\"Baseline Logistic (AUC={roc_auc_score(y_test,y_prob1):.2f})\",color=\"blue\")\n",
        "plt.plot(fpr2,tpr2,label=f\"Improved Logistic (AUC={roc_auc_score(y_test,y_prob2):.2f})\",color=\"green\")\n",
        "plt.plot(fpr_rf,tpr_rf,label=f\"Random Forest (AUC={roc_auc_score(y_test,y_prob_rf):.2f})\",color=\"red\")\n",
        "plt.plot(fpr_gb,tpr_gb,label=f\"Gradient Boosting (AUC={roc_auc_score(y_test,y_prob_gb):.2f})\",color=\"orange\")\n",
        "plt.plot([0,1],[0,1],\"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqADrThbVlxe"
      },
      "source": [
        "ROC curves confirm that both the improved logistic regression and gradient boosting outperform the baseline. Gradient boosting has the highest AUC (~0.65), indicating the strongest discriminative ability among the tested models, though performance is still moderate overall.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "22PxQpRD4qpS",
        "outputId": "0e87852b-be9f-4ca5-b736-a2f33251fd82"
      },
      "outputs": [],
      "source": [
        "# 8. Confusion Matrices\n",
        "# ================================\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum()\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm_norm, annot=cm, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Pred: Low\",\"Pred: High\"],\n",
        "                yticklabels=[\"True: Low\",\"True: High\"])\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each model\n",
        "plot_confusion_matrix(y_test, y_pred1, \"Baseline Logistic Regression\")\n",
        "plot_confusion_matrix(y_test, y_pred2, \"Improved Logistic Regression\")\n",
        "plot_confusion_matrix(y_test, y_pred_rf, \"Random Forest\")\n",
        "plot_confusion_matrix(y_test, y_pred_gb, \"Gradient Boosting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za50OBOoVprO"
      },
      "source": [
        "- Baseline logistic regression misclassified many negatives.  \n",
        "- Improved logistic regression balanced false positives and negatives better.  \n",
        "- Random Forest was stricter, reducing false positives at the cost of missed positives.  \n",
        "- Gradient Boosting struck the best balance, with fewer errors overall.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVfVGMC542bO",
        "outputId": "1b728427-129a-4b18-fae7-fc0dacc8adb1"
      },
      "outputs": [],
      "source": [
        "# 9. Model Performance Summary\n",
        "# ================================\n",
        "# Collect results\n",
        "baseline_results = [\n",
        "    accuracy_score(y_test,y_pred1),\n",
        "    precision_score(y_test,y_pred1),\n",
        "    recall_score(y_test,y_pred1),\n",
        "    f1_score(y_test,y_pred1),\n",
        "    roc_auc_score(y_test,y_prob1)\n",
        "]\n",
        "\n",
        "improved_results = [\n",
        "    accuracy_score(y_test,y_pred2),\n",
        "    precision_score(y_test,y_pred2),\n",
        "    recall_score(y_test,y_pred2),\n",
        "    f1_score(y_test,y_pred2),\n",
        "    roc_auc_score(y_test,y_prob2)\n",
        "]\n",
        "\n",
        "rf_results = [\n",
        "    accuracy_score(y_test,y_pred_rf),\n",
        "    precision_score(y_test,y_pred_rf),\n",
        "    recall_score(y_test,y_pred_rf),\n",
        "    f1_score(y_test,y_pred_rf),\n",
        "    roc_auc_score(y_test,y_prob_rf)\n",
        "]\n",
        "\n",
        "gb_results = [   # NEW\n",
        "    accuracy_score(y_test,y_pred_gb),\n",
        "    precision_score(y_test,y_pred_gb),\n",
        "    recall_score(y_test,y_pred_gb),\n",
        "    f1_score(y_test,y_pred_gb),\n",
        "    roc_auc_score(y_test,y_prob_gb)\n",
        "]\n",
        "\n",
        "# Combine results into summary table\n",
        "results = {\n",
        "    \"Model\":[\"Baseline Logistic\",\"Improved Logistic\",\"Random Forest\",\"Gradient Boosting\"],  # NEW\n",
        "    \"Accuracy\":[baseline_results[0], improved_results[0], rf_results[0], gb_results[0]],\n",
        "    \"Precision\":[baseline_results[1], improved_results[1], rf_results[1], gb_results[1]],\n",
        "    \"Recall\":[baseline_results[2], improved_results[2], rf_results[2], gb_results[2]],\n",
        "    \"F1 Score\":[baseline_results[3], improved_results[3], rf_results[3], gb_results[3]],\n",
        "    \"AUC\":[baseline_results[4], improved_results[4], rf_results[4], gb_results[4]]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nPerformance Summary:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aMlo54mWREtA",
        "outputId": "f047fb93-3a62-483a-efd1-41776c66c587"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "sns.heatmap(results_df.set_index(\"Model\"), annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Model Performance Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F74UjhV1VwwB"
      },
      "source": [
        "Comparing across models, Gradient Boosting emerges as the top performer, followed by Improved Logistic Regression. Random Forest underperformed in recall, while the baseline logistic model showed poor discrimination. Overall, supports clearly matter, but predictive power remains modest (AUC ~0.63–0.65)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eivvvW9I5KAC",
        "outputId": "785d822a-d797-4bd4-9b00-70de47b61060"
      },
      "outputs": [],
      "source": [
        "# 10. Cross-Validation (Optional but Strong)\n",
        "# ================================\n",
        "cv_scores = cross_val_score(LogisticRegression(max_iter=1000), X2, y, cv=5, scoring=\"roc_auc\")\n",
        "print(f\"\\nCross-validated AUC for Improved Logistic Regression: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
        "\n",
        "cv_scores_gb = cross_val_score(GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=42),\n",
        "                               X2, y, cv=5, scoring=\"roc_auc\")\n",
        "print(f\"Cross-validated AUC for Gradient Boosting: {cv_scores_gb.mean():.3f} ± {cv_scores_gb.std():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27aAEUk7VzYy"
      },
      "source": [
        "Cross-validation confirmed stability: Improved Logistic Regression averaged AUC ~0.61, and Gradient Boosting ~0.60. While not highly predictive, both models consistently beat chance, reinforcing the conclusion that workplace supports contribute to higher comfort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaF-mDhpV2aD"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This project investigated the role of workplace supports in shaping employees’ comfort discussing mental health.  \n",
        "\n",
        "- **Statistical Evidence:** A t-test showed employees with workplace resources scored significantly higher in comfort (mean difference = 0.44, p < 0.01).  \n",
        "- **Predictive Modeling:** Baseline logistic regression was weak, but adding employer discussion, medical coverage, combined supports, and demographics improved results. Gradient Boosting performed best (AUC ~0.65, recall ~0.81), showing modest predictive power.  \n",
        "- **Key Insights:** Employer-led discussions and medical coverage emerged as especially important predictors. Even modest supports, when combined, consistently increased comfort levels.  \n",
        "- **Limitations:** Predictive strength remains modest, indicating that unmeasured factors (e.g., workplace culture, stigma, personal experiences) also influence mental health comfort.  \n",
        "\n",
        "**Final Takeaway:** Workplace supports significantly improve employees’ comfort in discussing mental health, particularly when multiple supports are present. Employers can strengthen openness and well-being by offering accessible resources, fostering discussions, and ensuring medical coverage."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
